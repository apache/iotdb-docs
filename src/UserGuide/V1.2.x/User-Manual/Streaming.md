<!--

    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.

-->

# IoTDB Stream Processing Framework

The IoTDB stream processing framework allows users to implement customized stream processing logic, which can monitor and capture storage engine changes, transform changed data, and push transformed data outward.

We call <font color=RED>a data flow processing task a Pipe</font>. A stream processing task (Pipe) contains three subtasks:

- Extract
- Process
- Send (Connect)

The stream processing framework allows users to customize the processing logic of three subtasks using Java language and process data in a UDF-like manner.
In a Pipe, the three subtasks mentioned above are executed and implemented by three types of plugins. Data flows through these three plugins sequentially for processing:
Pipe Extractor is used to extract data, Pipe Processor is used to process data, Pipe Connector is used to send data, and the final data will be sent to an external system.

**The model for a Pipe task is as follows:**

![ä»»åŠ¡æ¨¡å‹å›¾](https://alioss.timecho.com/docs/img/%E5%90%8C%E6%AD%A5%E5%BC%95%E6%93%8E.jpeg)
A data stream processing task essentially describes the attributes of the Pipe Extractor, Pipe Processor, and Pipe Connector plugins.

Users can configure the specific attributes of these three subtasks declaratively using SQL statements. By combining different attributes, flexible data ETL (Extract, Transform, Load) capabilities can be achieved.

Using the stream processing framework, it is possible to build a complete data pipeline to fulfill various requirements such as *edge-to-cloud synchronization, remote disaster recovery, and read/write load balancing across multiple databases*.

## Custom Stream Processing Plugin Development

### Programming development dependencies

It is recommended to use Maven to build the project. Add the following dependencies in the `pom.xml` file. Please make sure to choose dependencies with the same version as the IoTDB server version.

```xml
<dependency>
    <groupId>org.apache.iotdb</groupId>
    <artifactId>pipe-api</artifactId>
    <version>1.2.1</version>
    <scope>provided</scope>
</dependency>
```

### Event-Driven Programming Model

The design of user programming interfaces for stream processing plugins follows the principles of the event-driven programming model. In this model, events serve as the abstraction of data in the user programming interface. The programming interface is decoupled from the specific execution method, allowing the focus to be on describing how the system expects events (data) to be processed upon arrival.

In the user programming interface of stream processing plugins, events abstract the write operations of database data. Events are captured by the local stream processing engine and passed sequentially through the three stages of stream processing, namely Pipe Extractor, Pipe Processor, and Pipe Connector plugins. User logic is triggered and executed within these three plugins.

To accommodate both low-latency stream processing in low-load scenarios and high-throughput stream processing in high-load scenarios at the edge, the stream processing engine dynamically chooses the processing objects from operation logs and data files. Therefore, the user programming interface for stream processing requires the user to provide the handling logic for two types of events: TabletInsertionEvent for operation log write events and TsFileInsertionEvent for data file write events.

#### **TabletInsertionEvent**

The TabletInsertionEvent is a high-level data abstraction for user write requests, which provides the ability to manipulate the underlying data of the write request by providing a unified operation interface.

For different database deployments, the underlying storage structure corresponding to the operation log write event is different. For stand-alone deployment scenarios, the operation log write event is an encapsulation of write-ahead log (WAL) entries; for distributed deployment scenarios, the operation log write event is an encapsulation of individual node consensus protocol operation log entries.

For write operations generated by different write request interfaces of the database, the data structure of the request structure corresponding to the operation log write event is also different.IoTDB provides many write interfaces such as InsertRecord, InsertRecords, InsertTablet, InsertTablets, and so on, and each kind of write request uses a completely different serialisation method to generate a write request. completely different serialisation methods and generate different binary entries.

The existence of operation log write events provides users with a unified view of data operations, which shields the implementation differences of the underlying data structures, greatly reduces the programming threshold for users, and improves the ease of use of the functionality.

```java
/** TabletInsertionEvent is used to define the event of data insertion. */
public interface TabletInsertionEvent extends Event {

  /**
   * The consumer processes the data row by row and collects the results by RowCollector.
   *
   * @return {@code Iterable<TabletInsertionEvent>} a list of new TabletInsertionEvent contains the
   *     results collected by the RowCollector
   */
  Iterable<TabletInsertionEvent> processRowByRow(BiConsumer<Row, RowCollector> consumer);

  /**
   * The consumer processes the Tablet directly and collects the results by RowCollector.
   *
   * @return {@code Iterable<TabletInsertionEvent>} a list of new TabletInsertionEvent contains the
   *     results collected by the RowCollector
   */
  Iterable<TabletInsertionEvent> processTablet(BiConsumer<Tablet, RowCollector> consumer);
}
```

#### **TsFileInsertionEvent**

The TsFileInsertionEvent represents a high-level abstraction of the database's disk flush operation and is a collection of multiple TabletInsertionEvents.

IoTDB's storage engine is based on the LSM (Log-Structured Merge) structure. When data is written, the write operations are first flushed to log-structured files, while the written data is also stored in memory. When the memory reaches its capacity limit, a flush operation is triggered, converting the data in memory into a database file while deleting the previously written log entries. During the conversion from memory data to database file data, two compression processes, encoding compression and universal compression, are applied. As a result, the data in the database file occupies less space compared to the original data in memory.

In extreme network conditions, directly transferring data files is more cost-effective than transmitting individual write operations. It consumes lower network bandwidth and achieves faster transmission speed. However, there is no such thing as a free lunch. Performing calculations on data in the disk file incurs additional costs for file I/O compared to performing calculations directly on data in memory. Nevertheless, the coexistence of disk data files and memory write operations permits dynamic trade-offs and adjustments. It is based on this observation that the data file write event is introduced into the event model of the plugin.

In summary, the data file write event appears in the event stream of stream processing plugins in the following two scenarios:

1. Historical data extraction: Before a stream processing task starts, all persisted write data exists in the form of TsFiles. When collecting historical data at the beginning of a stream processing task, the historical data is abstracted as TsFileInsertionEvent.

2. Real-time data extraction: During the execution of a stream processing task, if the speed of processing the log entries representing real-time operations is slower than the rate of write requests, the unprocessed log entries will be persisted to disk in the form of TsFiles. When these data are extracted by the stream processing engine, they are abstracted as TsFileInsertionEvent.

```java
/**
 * TsFileInsertionEvent is used to define the event of writing TsFile. Event data stores in disks,
 * which is compressed and encoded, and requires IO cost for computational processing.
 */
public interface TsFileInsertionEvent extends Event {

  /**
   * The method is used to convert the TsFileInsertionEvent into several TabletInsertionEvents.
   *
   * @return {@code Iterable<TabletInsertionEvent>} the list of TabletInsertionEvent
   */
  Iterable<TabletInsertionEvent> toTabletInsertionEvents();
}
```

### Custom Stream Processing Plugin Programming Interface Definition

Based on the custom stream processing plugin programming interface, users can easily write data extraction plugins, data processing plugins, and data sending plugins, allowing the stream processing functionality to adapt flexibly to various industrial scenarios.
#### Data Extraction Plugin Interface

Data extraction is the first stage of the three-stage process of stream processing, which includes data extraction, data processing, and data sending. The data extraction plugin (PipeExtractor) serves as a bridge between the stream processing engine and the storage engine. It captures various data write events by listening to the behavior of the storage engine.
```java
/**
 * PipeExtractor
 *
 * <p>PipeExtractor is responsible for capturing events from sources.
 *
 * <p>Various data sources can be supported by implementing different PipeExtractor classes.
 *
 * <p>The lifecycle of a PipeExtractor is as follows:
 *
 * <ul>
 *   <li>When a collaboration task is created, the KV pairs of `WITH EXTRACTOR` clause in SQL are
 *       parsed and the validation method {@link PipeExtractor#validate(PipeParameterValidator)}
 *       will be called to validate the parameters.
 *   <li>Before the collaboration task starts, the method {@link
 *       PipeExtractor#customize(PipeParameters, PipeExtractorRuntimeConfiguration)} will be called
 *       to config the runtime behavior of the PipeExtractor.
 *   <li>Then the method {@link PipeExtractor#start()} will be called to start the PipeExtractor.
 *   <li>While the collaboration task is in progress, the method {@link PipeExtractor#supply()} will
 *       be called to capture events from sources and then the events will be passed to the
 *       PipeProcessor.
 *   <li>The method {@link PipeExtractor#close()} will be called when the collaboration task is
 *       cancelled (the `DROP PIPE` command is executed).
 * </ul>
 */
public interface PipeExtractor extends PipePlugin {

  /**
   * This method is mainly used to validate {@link PipeParameters} and it is executed before {@link
   * PipeExtractor#customize(PipeParameters, PipeExtractorRuntimeConfiguration)} is called.
   *
   * @param validator the validator used to validate {@link PipeParameters}
   * @throws Exception if any parameter is not valid
   */
  void validate(PipeParameterValidator validator) throws Exception;

  /**
   * This method is mainly used to customize PipeExtractor. In this method, the user can do the
   * following things:
   *
   * <ul>
   *   <li>Use PipeParameters to parse key-value pair attributes entered by the user.
   *   <li>Set the running configurations in PipeExtractorRuntimeConfiguration.
   * </ul>
   *
   * <p>This method is called after the method {@link
   * PipeExtractor#validate(PipeParameterValidator)} is called.
   *
   * @param parameters used to parse the input parameters entered by the user
   * @param configuration used to set the required properties of the running PipeExtractor
   * @throws Exception the user can throw errors if necessary
   */
  void customize(PipeParameters parameters, PipeExtractorRuntimeConfiguration configuration)
          throws Exception;

  /**
   * Start the extractor. After this method is called, events should be ready to be supplied by
   * {@link PipeExtractor#supply()}. This method is called after {@link
   * PipeExtractor#customize(PipeParameters, PipeExtractorRuntimeConfiguration)} is called.
   *
   * @throws Exception the user can throw errors if necessary
   */
  void start() throws Exception;

  /**
   * Supply single event from the extractor and the caller will send the event to the processor.
   * This method is called after {@link PipeExtractor#start()} is called.
   *
   * @return the event to be supplied. the event may be null if the extractor has no more events at
   *     the moment, but the extractor is still running for more events.
   * @throws Exception the user can throw errors if necessary
   */
  Event supply() throws Exception;
}
```

#### Data Processing Plugin Interface

Data processing is the second stage of the three-stage process of stream processing, which includes data extraction, data processing, and data sending. The data processing plugin (PipeProcessor) is primarily used for filtering and transforming the various events captured by the data extraction plugin (PipeExtractor).

```java
/**
 * PipeProcessor
 *
 * <p>PipeProcessor is used to filter and transform the Event formed by the PipeExtractor.
 *
 * <p>The lifecycle of a PipeProcessor is as follows:
 *
 * <ul>
 *   <li>When a collaboration task is created, the KV pairs of `WITH PROCESSOR` clause in SQL are
 *       parsed and the validation method {@link PipeProcessor#validate(PipeParameterValidator)}
 *       will be called to validate the parameters.
 *   <li>Before the collaboration task starts, the method {@link
 *       PipeProcessor#customize(PipeParameters, PipeProcessorRuntimeConfiguration)} will be called
 *       to config the runtime behavior of the PipeProcessor.
 *   <li>While the collaboration task is in progress:
 *       <ul>
 *         <li>PipeExtractor captures the events and wraps them into three types of Event instances.
 *         <li>PipeProcessor processes the event and then passes them to the PipeConnector. The
 *             following 3 methods will be called: {@link
 *             PipeProcessor#process(TabletInsertionEvent, EventCollector)}, {@link
 *             PipeProcessor#process(TsFileInsertionEvent, EventCollector)} and {@link
 *             PipeProcessor#process(Event, EventCollector)}.
 *         <li>PipeConnector serializes the events into binaries and send them to sinks.
 *       </ul>
 *   <li>When the collaboration task is cancelled (the `DROP PIPE` command is executed), the {@link
 *       PipeProcessor#close() } method will be called.
 * </ul>
 */
public interface PipeProcessor extends PipePlugin {

  /**
   * This method is mainly used to validate {@link PipeParameters} and it is executed before {@link
   * PipeProcessor#customize(PipeParameters, PipeProcessorRuntimeConfiguration)} is called.
   *
   * @param validator the validator used to validate {@link PipeParameters}
   * @throws Exception if any parameter is not valid
   */
  void validate(PipeParameterValidator validator) throws Exception;

  /**
   * This method is mainly used to customize PipeProcessor. In this method, the user can do the
   * following things:
   *
   * <ul>
   *   <li>Use PipeParameters to parse key-value pair attributes entered by the user.
   *   <li>Set the running configurations in PipeProcessorRuntimeConfiguration.
   * </ul>
   *
   * <p>This method is called after the method {@link
   * PipeProcessor#validate(PipeParameterValidator)} is called and before the beginning of the
   * events processing.
   *
   * @param parameters used to parse the input parameters entered by the user
   * @param configuration used to set the required properties of the running PipeProcessor
   * @throws Exception the user can throw errors if necessary
   */
  void customize(PipeParameters parameters, PipeProcessorRuntimeConfiguration configuration)
          throws Exception;

  /**
   * This method is called to process the TabletInsertionEvent.
   *
   * @param tabletInsertionEvent TabletInsertionEvent to be processed
   * @param eventCollector used to collect result events after processing
   * @throws Exception the user can throw errors if necessary
   */
  void process(TabletInsertionEvent tabletInsertionEvent, EventCollector eventCollector)
          throws Exception;

  /**
   * This method is called to process the TsFileInsertionEvent.
   *
   * @param tsFileInsertionEvent TsFileInsertionEvent to be processed
   * @param eventCollector used to collect result events after processing
   * @throws Exception the user can throw errors if necessary
   */
  default void process(TsFileInsertionEvent tsFileInsertionEvent, EventCollector eventCollector)
          throws Exception {
    for (final TabletInsertionEvent tabletInsertionEvent :
            tsFileInsertionEvent.toTabletInsertionEvents()) {
      process(tabletInsertionEvent, eventCollector);
    }
  }

  /**
   * This method is called to process the Event.
   *
   * @param event Event to be processed
   * @param eventCollector used to collect result events after processing
   * @throws Exception the user can throw errors if necessary
   */
  void process(Event event, EventCollector eventCollector) throws Exception;
}
```

#### Data Sending Plugin Interface

Data sending is the third stage of the three-stage process of stream processing, which includes data extraction, data processing, and data sending. The data sending plugin (PipeConnector) is responsible for sending the various events processed by the data processing plugin (PipeProcessor). It serves as the network implementation layer of the stream processing framework and should support multiple real-time communication protocols and connectors in its interface.

```java
/**
 * PipeConnector
 *
 * <p>PipeConnector is responsible for sending events to sinks.
 *
 * <p>Various network protocols can be supported by implementing different PipeConnector classes.
 *
 * <p>The lifecycle of a PipeConnector is as follows:
 *
 * <ul>
 *   <li>When a collaboration task is created, the KV pairs of `WITH CONNECTOR` clause in SQL are
 *       parsed and the validation method {@link PipeConnector#validate(PipeParameterValidator)}
 *       will be called to validate the parameters.
 *   <li>Before the collaboration task starts, the method {@link
 *       PipeConnector#customize(PipeParameters, PipeConnectorRuntimeConfiguration)} will be called
 *       to config the runtime behavior of the PipeConnector and the method {@link
 *       PipeConnector#handshake()} will be called to create a connection with sink.
 *   <li>While the collaboration task is in progress:
 *       <ul>
 *         <li>PipeExtractor captures the events and wraps them into three types of Event instances.
 *         <li>PipeProcessor processes the event and then passes them to the PipeConnector.
 *         <li>PipeConnector serializes the events into binaries and send them to sinks. The
 *             following 3 methods will be called: {@link
 *             PipeConnector#transfer(TabletInsertionEvent)}, {@link
 *             PipeConnector#transfer(TsFileInsertionEvent)} and {@link
 *             PipeConnector#transfer(Event)}.
 *       </ul>
 *   <li>When the collaboration task is cancelled (the `DROP PIPE` command is executed), the {@link
 *       PipeConnector#close() } method will be called.
 * </ul>
 *
 * <p>In addition, the method {@link PipeConnector#heartbeat()} will be called periodically to check
 * whether the connection with sink is still alive. The method {@link PipeConnector#handshake()}
 * will be called to create a new connection with the sink when the method {@link
 * PipeConnector#heartbeat()} throws exceptions.
 */
public interface PipeConnector extends PipePlugin {

  /**
   * This method is mainly used to validate {@link PipeParameters} and it is executed before {@link
   * PipeConnector#customize(PipeParameters, PipeConnectorRuntimeConfiguration)} is called.
   *
   * @param validator the validator used to validate {@link PipeParameters}
   * @throws Exception if any parameter is not valid
   */
  void validate(PipeParameterValidator validator) throws Exception;

  /**
   * This method is mainly used to customize PipeConnector. In this method, the user can do the
   * following things:
   *
   * <ul>
   *   <li>Use PipeParameters to parse key-value pair attributes entered by the user.
   *   <li>Set the running configurations in PipeConnectorRuntimeConfiguration.
   * </ul>
   *
   * <p>This method is called after the method {@link
   * PipeConnector#validate(PipeParameterValidator)} is called and before the method {@link
   * PipeConnector#handshake()} is called.
   *
   * @param parameters used to parse the input parameters entered by the user
   * @param configuration used to set the required properties of the running PipeConnector
   * @throws Exception the user can throw errors if necessary
   */
  void customize(PipeParameters parameters, PipeConnectorRuntimeConfiguration configuration)
          throws Exception;

  /**
   * This method is used to create a connection with sink. This method will be called after the
   * method {@link PipeConnector#customize(PipeParameters, PipeConnectorRuntimeConfiguration)} is
   * called or will be called when the method {@link PipeConnector#heartbeat()} throws exceptions.
   *
   * @throws Exception if the connection is failed to be created
   */
  void handshake() throws Exception;

  /**
   * This method will be called periodically to check whether the connection with sink is still
   * alive.
   *
   * @throws Exception if the connection dies
   */
  void heartbeat() throws Exception;

  /**
   * This method is used to transfer the TabletInsertionEvent.
   *
   * @param tabletInsertionEvent TabletInsertionEvent to be transferred
   * @throws PipeConnectionException if the connection is broken
   * @throws Exception the user can throw errors if necessary
   */
  void transfer(TabletInsertionEvent tabletInsertionEvent) throws Exception;

  /**
   * This method is used to transfer the TsFileInsertionEvent.
   *
   * @param tsFileInsertionEvent TsFileInsertionEvent to be transferred
   * @throws PipeConnectionException if the connection is broken
   * @throws Exception the user can throw errors if necessary
   */
  default void transfer(TsFileInsertionEvent tsFileInsertionEvent) throws Exception {
    for (final TabletInsertionEvent tabletInsertionEvent :
            tsFileInsertionEvent.toTabletInsertionEvents()) {
      transfer(tabletInsertionEvent);
    }
  }

  /**
   * This method is used to transfer the Event.
   *
   * @param event Event to be transferred
   * @throws PipeConnectionException if the connection is broken
   * @throws Exception the user can throw errors if necessary
   */
  void transfer(Event event) throws Exception;
}
```

## Custom Stream Processing Plugin Management

To ensure the flexibility and usability of user-defined plugins in production environments, the system needs to provide the capability to dynamically manage plugins. This section introduces the management statements for stream processing plugins, which enable the dynamic and unified management of plugins.

### Load Plugin Statement

In IoTDB, to dynamically load a user-defined plugin into the system, you first need to implement a specific plugin class based on PipeExtractor, PipeProcessor, or PipeConnector. Then, you need to compile and package the plugin class into an executable jar file. Finally, you can use the loading plugin management statement to load the plugin into IoTDB.

The syntax of the loading plugin management statement is as follows:

```sql
CREATE PIPEPLUGIN <alias>
AS <Full class name>
USING <URL of jar file>
```

For example, if a user implements a data processing plugin with the fully qualified class name "edu.tsinghua.iotdb.pipe.ExampleProcessor" and packages it into a jar file, which is stored at "https://example.com:8080/iotdb/pipe-plugin.jar", and the user wants to use this plugin in the stream processing engine, marking the plugin as "example". The creation statement for this data processing plugin is as follows:

```sql
CREATE PIPEPLUGIN example
AS 'edu.tsinghua.iotdb.pipe.ExampleProcessor'
USING URI '<https://example.com:8080/iotdb/pipe-plugin.jar>'
```

### åˆ é™¤æ’ä»¶è¯­å¥

å½“ç”¨æˆ·ä¸å†æƒ³ä½¿ç”¨ä¸€ä¸ªæ’ä»¶ï¼Œéœ€è¦å°†æ’ä»¶ä»ç³»ç»Ÿä¸­å¸è½½æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¦‚å›¾æ‰€ç¤ºçš„åˆ é™¤æ’ä»¶è¯­å¥ã€‚

```sql
DROP PIPEPLUGIN <åˆ«å>
```

### æŸ¥çœ‹æ’ä»¶è¯­å¥

ç”¨æˆ·ä¹Ÿå¯ä»¥æŒ‰éœ€æŸ¥çœ‹ç³»ç»Ÿä¸­çš„æ’ä»¶ã€‚æŸ¥çœ‹æ’ä»¶çš„è¯­å¥å¦‚å›¾æ‰€ç¤ºã€‚

```sql
SHOW PIPEPLUGINS
```

## ç³»ç»Ÿé¢„ç½®çš„æµå¤„ç†æ’ä»¶

### é¢„ç½® extractor æ’ä»¶

#### iotdb-extractor

ä½œç”¨ï¼šæŠ½å– IoTDB å†…éƒ¨çš„å†å²æˆ–å®æ—¶æ•°æ®è¿›å…¥ pipeã€‚


| key                          | value                                            | value å–å€¼èŒƒå›´                         | required or optional with default |
| ---------------------------- | ------------------------------------------------ | -------------------------------------- | --------------------------------- |
| extractor                    | iotdb-extractor                                  | String: iotdb-extractor                | required                          |
| extractor.pattern            | ç”¨äºç­›é€‰æ—¶é—´åºåˆ—çš„è·¯å¾„å‰ç¼€                       | String: ä»»æ„çš„æ—¶é—´åºåˆ—å‰ç¼€             | optional: root                    |
| extractor.history.enable     | æ˜¯å¦æŠ½å–å†å²æ•°æ®                                 | Boolean: true, false                   | optional: true                    |
| extractor.history.start-time | æŠ½å–çš„å†å²æ•°æ®çš„å¼€å§‹ event timeï¼ŒåŒ…å« start-time | Long: [Long.MIN_VALUE, Long.MAX_VALUE] | optional: Long.MIN_VALUE          |
| extractor.history.end-time   | æŠ½å–çš„å†å²æ•°æ®çš„ç»“æŸ event timeï¼ŒåŒ…å« end-time   | Long: [Long.MIN_VALUE, Long.MAX_VALUE] | optional: Long.MAX_VALUE          |
| extractor.realtime.enable    | æ˜¯å¦æŠ½å–å®æ—¶æ•°æ®                                 | Boolean: true, false                   | optional: true                    |

> ğŸš« **extractor.pattern å‚æ•°è¯´æ˜**
>
> * Pattern éœ€ç”¨åå¼•å·ä¿®é¥°ä¸åˆæ³•å­—ç¬¦æˆ–è€…æ˜¯ä¸åˆæ³•è·¯å¾„èŠ‚ç‚¹ï¼Œä¾‹å¦‚å¦‚æœå¸Œæœ›ç­›é€‰ root.\`a@b\` æˆ–è€… root.\`123\`ï¼Œåº”è®¾ç½® pattern ä¸º root.\`a@b\` æˆ–è€… root.\`123\`ï¼ˆå…·ä½“å‚è€ƒ [å•åŒå¼•å·å’Œåå¼•å·çš„ä½¿ç”¨æ—¶æœº](https://iotdb.apache.org/zh/Download/#_1-0-ç‰ˆæœ¬ä¸å…¼å®¹çš„è¯­æ³•è¯¦ç»†è¯´æ˜)ï¼‰
> * åœ¨åº•å±‚å®ç°ä¸­ï¼Œå½“æ£€æµ‹åˆ° pattern ä¸º rootï¼ˆé»˜è®¤å€¼ï¼‰æ—¶ï¼ŒæŠ½å–æ•ˆç‡è¾ƒé«˜ï¼Œå…¶ä»–ä»»æ„æ ¼å¼éƒ½å°†é™ä½æ€§èƒ½
> * è·¯å¾„å‰ç¼€ä¸éœ€è¦èƒ½å¤Ÿæ„æˆå®Œæ•´çš„è·¯å¾„ã€‚ä¾‹å¦‚ï¼Œå½“åˆ›å»ºä¸€ä¸ªåŒ…å«å‚æ•°ä¸º 'extractor.pattern'='root.aligned.1' çš„ pipe æ—¶ï¼š
>
>   * root.aligned.1TS
>   * root.aligned.1TS.\`1\`
>   * root.aligned.100T
>   
>   çš„æ•°æ®ä¼šè¢«æŠ½å–ï¼›
>   
>   * root.aligned.\`1\`
>   * root.aligned.\`123\`
>
>   çš„æ•°æ®ä¸ä¼šè¢«æŠ½å–ã€‚

> â—ï¸**extractor.history çš„ start-timeï¼Œend-time å‚æ•°è¯´æ˜**
>
> * start-timeï¼Œend-time åº”ä¸º ISO æ ¼å¼ï¼Œä¾‹å¦‚ 2011-12-03T10:15:30 æˆ– 2011-12-03T10:15:30+01:00

> âœ… **ä¸€æ¡æ•°æ®ä»ç”Ÿäº§åˆ°è½åº“ IoTDBï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®çš„æ—¶é—´æ¦‚å¿µ**
>
> * **event timeï¼š** æ•°æ®å®é™…ç”Ÿäº§æ—¶çš„æ—¶é—´ï¼ˆæˆ–è€…æ•°æ®ç”Ÿäº§ç³»ç»Ÿç»™æ•°æ®èµ‹äºˆçš„ç”Ÿæˆæ—¶é—´ï¼Œæ˜¯æ•°æ®ç‚¹ä¸­çš„æ—¶é—´é¡¹ï¼‰ï¼Œä¹Ÿç§°ä¸ºäº‹ä»¶æ—¶é—´ã€‚
> * **arrival timeï¼š** æ•°æ®åˆ°è¾¾ IoTDB ç³»ç»Ÿå†…çš„æ—¶é—´ã€‚
>
> æˆ‘ä»¬å¸¸è¯´çš„ä¹±åºæ•°æ®ï¼ŒæŒ‡çš„æ˜¯æ•°æ®åˆ°è¾¾æ—¶ï¼Œå…¶ **event time** è¿œè½åäºå½“å‰ç³»ç»Ÿæ—¶é—´ï¼ˆæˆ–è€…å·²ç»è½åº“çš„æœ€å¤§ **event time**ï¼‰çš„æ•°æ®ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸è®ºæ˜¯ä¹±åºæ•°æ®è¿˜æ˜¯é¡ºåºæ•°æ®ï¼Œåªè¦å®ƒä»¬æ˜¯æ–°åˆ°è¾¾ç³»ç»Ÿçš„ï¼Œé‚£å®ƒä»¬çš„ **arrival time** éƒ½æ˜¯ä¼šéšç€æ•°æ®åˆ°è¾¾ IoTDB çš„é¡ºåºé€’å¢çš„ã€‚

> ğŸ’ **iotdb-extractor çš„å·¥ä½œå¯ä»¥æ‹†åˆ†æˆä¸¤ä¸ªé˜¶æ®µ**
>
> 1. å†å²æ•°æ®æŠ½å–ï¼šæ‰€æœ‰ **arrival time** < åˆ›å»º pipe æ—¶**å½“å‰ç³»ç»Ÿæ—¶é—´**çš„æ•°æ®ç§°ä¸ºå†å²æ•°æ®
> 2. å®æ—¶æ•°æ®æŠ½å–ï¼šæ‰€æœ‰ **arrival time** >= åˆ›å»º pipe æ—¶**å½“å‰ç³»ç»Ÿæ—¶é—´**çš„æ•°æ®ç§°ä¸ºå®æ—¶æ•°æ®
>
> å†å²æ•°æ®ä¼ è¾“é˜¶æ®µå’Œå®æ—¶æ•°æ®ä¼ è¾“é˜¶æ®µï¼Œ**ä¸¤é˜¶æ®µä¸²è¡Œæ‰§è¡Œï¼Œåªæœ‰å½“å†å²æ•°æ®ä¼ è¾“é˜¶æ®µå®Œæˆåï¼Œæ‰æ‰§è¡Œå®æ—¶æ•°æ®ä¼ è¾“é˜¶æ®µã€‚**
>
> ç”¨æˆ·å¯ä»¥æŒ‡å®š iotdb-extractor è¿›è¡Œï¼š
>
> * å†å²æ•°æ®æŠ½å–ï¼ˆ`'extractor.history.enable' = 'true'`, `'extractor.realtime.enable' = 'false'` ï¼‰
> * å®æ—¶æ•°æ®æŠ½å–ï¼ˆ`'extractor.history.enable' = 'false'`, `'extractor.realtime.enable' = 'true'` ï¼‰
> * å…¨é‡æ•°æ®æŠ½å–ï¼ˆ`'extractor.history.enable' = 'true'`, `'extractor.realtime.enable' = 'true'` ï¼‰
> * ç¦æ­¢åŒæ—¶è®¾ç½® `extractor.history.enable` å’Œ `extractor.realtime.enable` ä¸º `false`

### é¢„ç½® processor æ’ä»¶

#### do-nothing-processor

ä½œç”¨ï¼šä¸å¯¹ extractor ä¼ å…¥çš„äº‹ä»¶åšä»»ä½•çš„å¤„ç†ã€‚


| key       | value                | value å–å€¼èŒƒå›´               | required or optional with default |
| --------- | -------------------- | ---------------------------- | --------------------------------- |
| processor | do-nothing-processor | String: do-nothing-processor | required                          |

### é¢„ç½® connector æ’ä»¶

#### do-nothing-connector

ä½œç”¨ï¼šä¸å¯¹ processor ä¼ å…¥çš„äº‹ä»¶åšä»»ä½•çš„å¤„ç†ã€‚


| key       | value                | value å–å€¼èŒƒå›´               | required or optional with default |
| --------- | -------------------- | ---------------------------- | --------------------------------- |
| connector | do-nothing-connector | String: do-nothing-connector | required                          |

## æµå¤„ç†ä»»åŠ¡ç®¡ç†

### åˆ›å»ºæµå¤„ç†ä»»åŠ¡

ä½¿ç”¨ `CREATE PIPE` è¯­å¥æ¥åˆ›å»ºæµå¤„ç†ä»»åŠ¡ã€‚ä»¥æ•°æ®åŒæ­¥æµå¤„ç†ä»»åŠ¡çš„åˆ›å»ºä¸ºä¾‹ï¼Œç¤ºä¾‹ SQL è¯­å¥å¦‚ä¸‹ï¼š

```sql
CREATE PIPE <PipeId> -- PipeId æ˜¯èƒ½å¤Ÿå”¯ä¸€æ ‡å®šæµå¤„ç†ä»»åŠ¡çš„åå­—
WITH EXTRACTOR (
  -- é»˜è®¤çš„ IoTDB æ•°æ®æŠ½å–æ’ä»¶
  'extractor'                    = 'iotdb-extractor',
  -- è·¯å¾„å‰ç¼€ï¼Œåªæœ‰èƒ½å¤ŸåŒ¹é…è¯¥è·¯å¾„å‰ç¼€çš„æ•°æ®æ‰ä¼šè¢«æŠ½å–ï¼Œç”¨ä½œåç»­çš„å¤„ç†å’Œå‘é€
  'extractor.pattern'            = 'root.timecho',
  -- æ˜¯å¦æŠ½å–å†å²æ•°æ®
  'extractor.history.enable'     = 'true',
  -- æè¿°è¢«æŠ½å–çš„å†å²æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼Œè¡¨ç¤ºæœ€æ—©æ—¶é—´
  'extractor.history.start-time' = '2011.12.03T10:15:30+01:00',
  -- æè¿°è¢«æŠ½å–çš„å†å²æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼Œè¡¨ç¤ºæœ€æ™šæ—¶é—´
  'extractor.history.end-time'   = '2022.12.03T10:15:30+01:00',
  -- æ˜¯å¦æŠ½å–å®æ—¶æ•°æ®
  'extractor.realtime.enable'    = 'true',
)
WITH PROCESSOR (
  -- é»˜è®¤çš„æ•°æ®å¤„ç†æ’ä»¶ï¼Œå³ä¸åšä»»ä½•å¤„ç†
  'processor'                    = 'do-nothing-processor',
)
WITH CONNECTOR (
  -- IoTDB æ•°æ®å‘é€æ’ä»¶ï¼Œç›®æ ‡ç«¯ä¸º IoTDB
  'connector'                    = 'iotdb-thrift-connector',
  -- ç›®æ ‡ç«¯ IoTDB å…¶ä¸­ä¸€ä¸ª DataNode èŠ‚ç‚¹çš„æ•°æ®æœåŠ¡ ip
  'connector.ip'                 = '127.0.0.1',
  -- ç›®æ ‡ç«¯ IoTDB å…¶ä¸­ä¸€ä¸ª DataNode èŠ‚ç‚¹çš„æ•°æ®æœåŠ¡ port
  'connector.port'               = '6667',
)
```

**åˆ›å»ºæµå¤„ç†ä»»åŠ¡æ—¶éœ€è¦é…ç½® PipeId ä»¥åŠä¸‰ä¸ªæ’ä»¶éƒ¨åˆ†çš„å‚æ•°ï¼š**


| é…ç½®é¡¹    | è¯´æ˜                                                | æ˜¯å¦å¿…å¡«                    | é»˜è®¤å®ç°             | é»˜è®¤å®ç°è¯´æ˜                                             | æ˜¯å¦å…è®¸è‡ªå®šä¹‰å®ç°        |
| --------- | --------------------------------------------------- | --------------------------- | -------------------- | -------------------------------------------------------- | ------------------------- |
| PipeId    | å…¨å±€å”¯ä¸€æ ‡å®šä¸€ä¸ªæµå¤„ç†ä»»åŠ¡çš„åç§°                    | <font color=red>å¿…å¡«</font> | -                    | -                                                        | -                         |
| extractor | Pipe Extractor æ’ä»¶ï¼Œè´Ÿè´£åœ¨æ•°æ®åº“åº•å±‚æŠ½å–æµå¤„ç†æ•°æ® | é€‰å¡«                        | iotdb-extractor      | å°†æ•°æ®åº“çš„å…¨é‡å†å²æ•°æ®å’Œåç»­åˆ°è¾¾çš„å®æ—¶æ•°æ®æ¥å…¥æµå¤„ç†ä»»åŠ¡ | å¦                        |
| processor | Pipe Processor æ’ä»¶ï¼Œè´Ÿè´£å¤„ç†æ•°æ®                   | é€‰å¡«                        | do-nothing-processor | å¯¹ä¼ å…¥çš„æ•°æ®ä¸åšä»»ä½•å¤„ç†                                 | <font color=red>æ˜¯</font> |
| connector | Pipe Connector æ’ä»¶ï¼Œè´Ÿè´£å‘é€æ•°æ®                   | <font color=red>å¿…å¡«</font> | -                    | -                                                        | <font color=red>æ˜¯</font> |

ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨äº† iotdb-extractorã€do-nothing-processor å’Œ iotdb-thrift-connector æ’ä»¶æ„å»ºæ•°æ®æµå¤„ç†ä»»åŠ¡ã€‚IoTDB è¿˜å†…ç½®äº†å…¶ä»–çš„æµå¤„ç†æ’ä»¶ï¼Œ**è¯·æŸ¥çœ‹â€œç³»ç»Ÿé¢„ç½®æµå¤„ç†æ’ä»¶â€ä¸€èŠ‚**ã€‚

**ä¸€ä¸ªæœ€ç®€çš„ CREATE PIPE è¯­å¥ç¤ºä¾‹å¦‚ä¸‹ï¼š**

```sql
CREATE PIPE <PipeId> -- PipeId æ˜¯èƒ½å¤Ÿå”¯ä¸€æ ‡å®šæµå¤„ç†ä»»åŠ¡çš„åå­—
WITH CONNECTOR (
  -- IoTDB æ•°æ®å‘é€æ’ä»¶ï¼Œç›®æ ‡ç«¯ä¸º IoTDB
  'connector'      = 'iotdb-thrift-connector',
  -- ç›®æ ‡ç«¯ IoTDB å…¶ä¸­ä¸€ä¸ª DataNode èŠ‚ç‚¹çš„æ•°æ®æœåŠ¡ ip
  'connector.ip'   = '127.0.0.1',
  -- ç›®æ ‡ç«¯ IoTDB å…¶ä¸­ä¸€ä¸ª DataNode èŠ‚ç‚¹çš„æ•°æ®æœåŠ¡ port
  'connector.port' = '6667',
)
```

å…¶è¡¨è¾¾çš„è¯­ä¹‰æ˜¯ï¼šå°†æœ¬æ•°æ®åº“å®ä¾‹ä¸­çš„å…¨é‡å†å²æ•°æ®å’Œåç»­åˆ°è¾¾çš„å®æ—¶æ•°æ®ï¼ŒåŒæ­¥åˆ°ç›®æ ‡ä¸º 127.0.0.1:6667 çš„ IoTDB å®ä¾‹ä¸Šã€‚

**æ³¨æ„ï¼š**

- EXTRACTOR å’Œ PROCESSOR ä¸ºé€‰å¡«é…ç½®ï¼Œè‹¥ä¸å¡«å†™é…ç½®å‚æ•°ï¼Œç³»ç»Ÿåˆ™ä¼šé‡‡ç”¨ç›¸åº”çš„é»˜è®¤å®ç°
- CONNECTOR ä¸ºå¿…å¡«é…ç½®ï¼Œéœ€è¦åœ¨ CREATE PIPE è¯­å¥ä¸­å£°æ˜å¼é…ç½®
- CONNECTOR å…·å¤‡è‡ªå¤ç”¨èƒ½åŠ›ã€‚å¯¹äºä¸åŒçš„æµå¤„ç†ä»»åŠ¡ï¼Œå¦‚æœä»–ä»¬çš„ CONNECTOR å…·å¤‡å®Œå…¨ç›¸åŒ KV å±æ€§çš„ï¼ˆæ‰€æœ‰å±æ€§çš„ key å¯¹åº”çš„ value éƒ½ç›¸åŒï¼‰ï¼Œ**é‚£ä¹ˆç³»ç»Ÿæœ€ç»ˆåªä¼šåˆ›å»ºä¸€ä¸ª CONNECTOR å®ä¾‹**ï¼Œä»¥å®ç°å¯¹è¿æ¥èµ„æºçš„å¤ç”¨ã€‚

  - ä¾‹å¦‚ï¼Œæœ‰ä¸‹é¢ pipe1, pipe2 ä¸¤ä¸ªæµå¤„ç†ä»»åŠ¡çš„å£°æ˜ï¼š

  ```sql
  CREATE PIPE pipe1
  WITH CONNECTOR (
    'connector' = 'iotdb-thrift-connector',
    'connector.thrift.host' = 'localhost',
    'connector.thrift.port' = '9999',
  )

  CREATE PIPE pipe2
  WITH CONNECTOR (
    'connector' = 'iotdb-thrift-connector',
    'connector.thrift.port' = '9999',
    'connector.thrift.host' = 'localhost',
  )
  ```

  - å› ä¸ºå®ƒä»¬å¯¹ CONNECTOR çš„å£°æ˜å®Œå…¨ç›¸åŒï¼ˆ**å³ä½¿æŸäº›å±æ€§å£°æ˜æ—¶çš„é¡ºåºä¸åŒ**ï¼‰ï¼Œæ‰€ä»¥æ¡†æ¶ä¼šè‡ªåŠ¨å¯¹å®ƒä»¬å£°æ˜çš„ CONNECTOR è¿›è¡Œå¤ç”¨ï¼Œæœ€ç»ˆ pipe1, pipe2 çš„CONNECTOR å°†ä¼šæ˜¯åŒä¸€ä¸ªå®ä¾‹ã€‚
- è¯·ä¸è¦æ„å»ºå‡ºåŒ…å«æ•°æ®å¾ªç¯åŒæ­¥çš„åº”ç”¨åœºæ™¯ï¼ˆä¼šå¯¼è‡´æ— é™å¾ªç¯ï¼‰ï¼š

  - IoTDB A -> IoTDB B -> IoTDB A
  - IoTDB A -> IoTDB A

### å¯åŠ¨æµå¤„ç†ä»»åŠ¡

CREATE PIPE è¯­å¥æˆåŠŸæ‰§è¡Œåï¼Œæµå¤„ç†ä»»åŠ¡ç›¸å…³å®ä¾‹ä¼šè¢«åˆ›å»ºï¼Œä½†æ•´ä¸ªæµå¤„ç†ä»»åŠ¡çš„è¿è¡ŒçŠ¶æ€ä¼šè¢«ç½®ä¸º STOPPEDï¼Œå³æµå¤„ç†ä»»åŠ¡ä¸ä¼šç«‹åˆ»å¤„ç†æ•°æ®ã€‚

å¯ä»¥ä½¿ç”¨ START PIPE è¯­å¥ä½¿æµå¤„ç†ä»»åŠ¡å¼€å§‹å¤„ç†æ•°æ®ï¼š

```sql
START PIPE <PipeId>
```

### åœæ­¢æµå¤„ç†ä»»åŠ¡

ä½¿ç”¨ STOP PIPE è¯­å¥ä½¿æµå¤„ç†ä»»åŠ¡åœæ­¢å¤„ç†æ•°æ®ï¼š

```sql
STOP PIPE <PipeId>
```

### åˆ é™¤æµå¤„ç†ä»»åŠ¡

ä½¿ç”¨ DROP PIPE è¯­å¥ä½¿æµå¤„ç†ä»»åŠ¡åœæ­¢å¤„ç†æ•°æ®ï¼ˆå½“æµå¤„ç†ä»»åŠ¡çŠ¶æ€ä¸º RUNNING æ—¶ï¼‰ï¼Œç„¶ååˆ é™¤æ•´ä¸ªæµå¤„ç†ä»»åŠ¡æµå¤„ç†ä»»åŠ¡ï¼š

```sql
DROP PIPE <PipeId>
```

ç”¨æˆ·åœ¨åˆ é™¤æµå¤„ç†ä»»åŠ¡å‰ï¼Œä¸éœ€è¦æ‰§è¡Œ STOP æ“ä½œã€‚

### å±•ç¤ºæµå¤„ç†ä»»åŠ¡

ä½¿ç”¨ SHOW PIPES è¯­å¥æŸ¥çœ‹æ‰€æœ‰æµå¤„ç†ä»»åŠ¡ï¼š

```sql
SHOW PIPES
```

æŸ¥è¯¢ç»“æœå¦‚ä¸‹ï¼š

```sql
+-----------+-----------------------+-------+-------------+-------------+-------------+----------------+
|         ID|          CreationTime |  State|PipeExtractor|PipeProcessor|PipeConnector|ExceptionMessage|
+-----------+-----------------------+-------+-------------+-------------+-------------+----------------+
|iotdb-kafka|2022-03-30T20:58:30.689|RUNNING|          ...|          ...|          ...|            None|
+-----------+-----------------------+-------+-------------+-------------+-------------+----------------+
|iotdb-iotdb|2022-03-31T12:55:28.129|STOPPED|          ...|          ...|          ...| TException: ...|
+-----------+-----------------------+-------+-------------+-------------+-------------+----------------+
```

å¯ä»¥ä½¿ç”¨ `<PipeId>` æŒ‡å®šæƒ³çœ‹çš„æŸä¸ªæµå¤„ç†ä»»åŠ¡çŠ¶æ€ï¼š

```sql
SHOW PIPE <PipeId>
```

æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ where å­å¥ï¼Œåˆ¤æ–­æŸä¸ª \<PipeId\> ä½¿ç”¨çš„ Pipe Connector è¢«å¤ç”¨çš„æƒ…å†µã€‚

```sql
SHOW PIPES
WHERE CONNECTOR USED BY <PipeId>
```

### Stream Processing Task Running Status Migration

A stream processing task status can transition through several states during the lifecycle of a data synchronization pipe:

- **STOPPEDï¼š** The pipe is in a stopped state. It can have the following possibilities:
  - After the successful creation of a pipe, its initial state is set to stopped
  - The user manually pauses a pipe that is in normal running state, transitioning its status from RUNNING to STOPPED
  - If a pipe encounters an unrecoverable error during execution, its status automatically changes from RUNNING to STOPPED.
- **RUNNINGï¼š** The pipe is actively processing data
- **DROPPEDï¼š** The pipe is permanently deleted

The following diagram illustrates the different states and their transitions:

![state migration diagram](https://alioss.timecho.com/docs/img/%E7%8A%B6%E6%80%81%E8%BF%81%E7%A7%BB%E5%9B%BE.png)

## Authority Management

### Stream Processing Task

| Authority Name    | Description                 |
| ----------- | -------------------- |
| CREATE_PIPE | Register task,path-independent |
| START_PIPE  | Start task,path-independent |
| STOP_PIPE   | Stop task,path-independent |
| DROP_PIPE   | Uninstall task,path-independent |
| SHOW_PIPES  | Query task,path-independent |
### Stream Processing Task Plugin


| Authority Name          | Description                           |
| ----------------- | ------------------------------ |
| CREATE_PIPEPLUGIN | Register stream processing task plugin,path-independent |
| DROP_PIPEPLUGIN   | Start stream processing task plugin,path-independent |
| SHOW_PIPEPLUGINS  | Query stream processing task plugin,path-independent |

## Configure Parameters

In iotdb-common.properties ï¼š

```Properties
####################
### Pipe Configuration
####################

# Uncomment the following field to configure the pipe lib directory.
# For Windows platform
# If its prefix is a drive specifier followed by "\\", or if its prefix is "\\\\", then the path is
# absolute. Otherwise, it is relative.
# pipe_lib_dir=ext\\pipe
# For Linux platform
# If its prefix is "/", then the path is absolute. Otherwise, it is relative.
# pipe_lib_dir=ext/pipe

# The maximum number of threads that can be used to execute the pipe subtasks in PipeSubtaskExecutor.
# The actual value will be min(pipe_subtask_executor_max_thread_num, max(1, CPU core number / 2)).
# pipe_subtask_executor_max_thread_num=5

# The connection timeout (in milliseconds) for the thrift client.
# pipe_connector_timeout_ms=900000
```