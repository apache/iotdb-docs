<!--

    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.

-->
# 时序大模型

## 1. 简介

时序大模型是专为时序数据分析设计的基础模型。IoTDB 团队长期自研时序基础模型 Timer，该模型基于 Transformer 架构，经海量多领域时序数据预训练，可支撑时序预测、异常检测、时序填补等下游任务；团队打造的 AINode 平台同时支持集成业界前沿时序基础模型，为用户提供多元选型。不同于传统时序分析技术，这类大模型具备通用特征提取能力，可通过零样本分析、微调等技术服务广泛的分析任务。

本文相关时序大模型领域的技术成果（含团队自研及业界前沿方向）均发表于国际机器学习顶级会议，具体内容见附录。

## 2. 应用场景

* **时序预测**：为工业生产、自然环境等领域提供时间序列数据的预测服务，帮助用户提前了解未来变化趋势。
* **数据填补**：针对时间序列中的缺失序列段，进行上下文填补，以增强数据集的连续性和完整性。
* **异常检测**：利用自回归分析技术，对时间序列数据进行实时监测，及时预警潜在的异常情况。

![](/img/LargeModel09.png)

## 3. Timer-1 模型

Timer<sup><a href="#appendix1" id="ref1" style="text-decoration: none;">[1]</a></sup> 模型（非内置模型）不仅展现了出色的少样本泛化和多任务适配能力，还通过预训练获得了丰富的知识库，赋予了它处理多样化下游任务的通用能力，拥有以下特点：

* **泛化性**：模型能够通过使用少量样本进行微调，达到行业内领先的深度模型预测效果。
* **通用性**：模型设计灵活，能够适配多种不同的任务需求，并且支持变化的输入和输出长度，使其在各种应用场景中都能发挥作用。
* **可扩展性**：随着模型参数数量的增加或预训练数据规模的扩大，模型效果会持续提升，确保模型能够随着时间和数据量的增长而不断优化其预测效果。

![](/img/model01.png)

## 4. Timer-XL 模型

Timer-XL<sup><a href="#appendix2" id="ref2" style="text-decoration: none;">[2]</a></sup>基于 Timer 进一步扩展升级了网络结构，在多个维度全面突破：

* **超长上下文支持**：该模型突破了传统时序预测模型的限制，支持处理数千个 Token（相当于数万个时间点）的输入，有效解决了上下文长度瓶颈问题。
* **多变量预测场景覆盖**：支持多种预测场景，包括非平稳时间序列的预测、涉及多个变量的预测任务以及包含协变量的预测，满足多样化的业务需求。
* **大规模工业时序数据集：**采用万亿大规模工业物联网领域的时序数据集进行预训练，数据集兼有庞大的体量、卓越的质量和丰富的领域等重要特质，覆盖能源、航空航天、钢铁、交通等多领域。

![](/img/model02.png)

## 5. Timer-Sundial 模型

Timer-Sundial<sup><a href="#appendix3" id="ref3" style="text-decoration: none;">[3]</a></sup>是一个专注于时间序列预测的生成式基础模型系列，其基础版本拥有 1.28 亿参数，并在 1 万亿个时间点上进行了大规模预训练，其核心特性包括：

* **强大的泛化性能：**具备零样本预测能力，可同时支持点预测和概率预测。
* **灵活预测分布分析：**不仅能预测均值或分位数，还可通过模型生成的原始样本评估预测分布的任意统计特性。
* **创新生成架构：** 采用 “Transformer + TimeFlow” 协同架构——Transformer 学习时间片段的自回归表征，TimeFlow 模块基于流匹配框架 (Flow-Matching) 将随机噪声转化为多样化预测轨迹，实现高效的非确定性样本生成。

![](/img/model03.png)

## 6. Chronos-2 模型

Chronos-2 <sup><a href="#appendix4" id="ref4" style="text-decoration: none;">[4]</a></sup>是由 Amazon Web Services (AWS) 研究团队开发的，基于 Chronos 离散词元建模范式发展起来的通用时间序列基础模型，该模型同时适用于零样本单变量预测和协变量预测。其主要特性包括：

* **概率性预测能力**：模型以生成式方式输出多步预测结果，支持分位数或分布级预测，从而刻画未来不确定性。
* **零样本通用预测**：依托预训练获得的上下文学习能力，可直接对未见过的数据集执行预测，无需重新训练或参数更新。
* **多变量与协变量统一建模**：支持在同一架构下联合建模多条相关时间序列及其协变量，以提升复杂任务的预测效果。但对输入有严格要求：
    * 未来协变量的名称组成的集合必须是历史协变量的名称组成的集合的子集；
    * 每个历史协变量的长度必须等于目标变量的长度；
    * 每个未来协变量的长度必须等于预测长度；
* **高效推理与部署**：模型采用紧凑的编码器式（encoder-only）结构，在保持强泛化能力的同时兼顾推理效率。

![](/img/timeseries-large-model-chronos2.png)

## 7. 效果展示

时序大模型能够适应多种不同领域和场景的真实时序数据，在各种任务上拥有优异的处理效果，以下是在不同数据上的真实表现：

**时序预测：**

利用时序大模型的预测能力，能够准确预测时间序列的未来变化趋势，如下图蓝色曲线代表预测趋势，红色曲线为实际趋势，两曲线高度吻合。

![](/img/LargeModel03.png)

**数据填补**：

利用时序大模型对缺失数据段进行预测式填补。

![](/img/timeseries-large-model-data-imputation.png)

**异常检测**：

利用时序大模型精准识别与正常趋势偏离过大的异常值。

![](/img/LargeModel05.png)

## 8. 部署使用

1. 打开 IoTDB cli 控制台，检查 ConfigNode、DataNode、AINode 节点确保均为 Running。

```Plain
IoTDB> show cluster
+------+----------+-------+---------------+------------+--------------+-----------+
|NodeID|  NodeType| Status|InternalAddress|InternalPort|       Version|  BuildInfo|
+------+----------+-------+---------------+------------+--------------+-----------+
|     0|ConfigNode|Running|      127.0.0.1|       10710|       2.0.5.1|    069354f|
|     1|  DataNode|Running|      127.0.0.1|       10730|       2.0.5.1|    069354f|
|     2|    AINode|Running|      127.0.0.1|       10810|       2.0.5.1|069354f-dev|
+------+----------+-------+---------------+------------+--------------+-----------+
Total line number = 3
It costs 0.140s
```

2. 联网环境下首次启动 AINode 节点会自动拉取 Timer-XL、Sundial、Chronos2 模型。

   > 注意：
   >
   > * AINode 安装包不包含模型权重文件
   > * 自动拉取功能依赖部署环境具备 HuggingFace 网络访问能力
   > * AINode 支持手动上传模型权重文件，具体操作方法可参考[导入权重文件](../Deployment-and-Maintenance/AINode_Deployment_Upgrade_timecho.md#_3-3-导入内置权重文件)

3. 检查模型是否可用。

```Bash
IoTDB> show models
+---------------------+---------+--------+--------+
|              ModelId|ModelType|Category|   State|
+---------------------+---------+--------+--------+
|                arima|   sktime| builtin|  active|
|          holtwinters|   sktime| builtin|  active|
|exponential_smoothing|   sktime| builtin|  active|
|     naive_forecaster|   sktime| builtin|  active|
|       stl_forecaster|   sktime| builtin|  active|
|         gaussian_hmm|   sktime| builtin|  active|
|              gmm_hmm|   sktime| builtin|  active|
|                stray|   sktime| builtin|  active|
|             timer_xl|    timer| builtin|  active|
|              sundial|  sundial| builtin|  active|
|             chronos2|       t5| builtin|  active|
+---------------------+---------+--------+--------+
```

### 附录

<a id="appendix1"></a>**[1]** Timer- Generative Pre-trained Transformers Are Large Time Series Models, Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long. [↩ 返回](<u>#ref1</u>)

<a id="appendix2"></a>**[2]** TIMER-XL- LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING ,Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long. [↩ 返回](<u>#ref2</u>)

<a id="appendix3"></a>**[3]** Sundial- A Family of Highly Capable Time Series Foundation Models, Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long, **ICML 2025 spotlight**. [↩ 返回](<u>#ref3</u>)

<a id="appendix4"></a>**[4] **Chronos-2: From Univariate to Universal Forecasting, Abdul Fatir Ansari, Oleksandr Shchur, Jaris Küken, Andreas Auer, Boran Han, Pedro Mercado, Syama Sundar Rangapuram, Huibin Shen, Lorenzo Stella, Xiyuan Zhang, Mononito Goswami, Shubham Kapoor, Danielle C. Maddix, Pablo Guerron, Tony Hu, Junming Yin, Nick Erickson, Prateek Mutalik Desai, Hao Wang, Huzefa Rangwala, George Karypis, Yuyang Wang, Michael Bohlke-Schneider, **arXiv:2510.15821.**[↩ 返回](<u>#ref4</u>)
